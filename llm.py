import os
import sys
from typing import List, Dict, Any, Optional
from datetime import datetime
from dotenv import load_dotenv
import time  # ì´ ì¤„ ì¶”ê°€!

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ Python pathì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
# llm.pyê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆìœ¼ë¯€ë¡œ current_dirì´ ë°”ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = current_dir
sys.path.insert(0, project_root)

print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {current_dir}")
print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
print(f"Python ê²½ë¡œì— ì¶”ê°€ë¨: {project_root}")

from langchain_openai import ChatOpenAI
from langchain.schema import Document, HumanMessage, SystemMessage
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from langchain.chains.retrieval_qa.base import BaseRetrievalQA

# ê¸°ì¡´ì— ë§Œë“  ëª¨ë“ˆë“¤ ì„í¬íŠ¸
try:
    from utils.document_processing import DocumentProcessor
    from utils.vector_store import SafeVectorStoreManager as VectorStoreManager
    print("âœ… ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print(f"utils í´ë” ê²½ë¡œ í™•ì¸: {os.path.join(project_root, 'utils')}")
    print(f"utils í´ë” ì¡´ì¬: {os.path.exists(os.path.join(project_root, 'utils'))}")
    raise

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class RAGPipeline:
    """
    RAG (Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
    ë²¡í„° ê²€ìƒ‰ + LLM ì‘ë‹µ ìƒì„±ì„ í†µí•© ê´€ë¦¬
    """
    
    def __init__(self, 
                 model_name: str = "gpt-4o-mini",
                 temperature: float = 0.1-0,
                 max_tokens: int = 400,
                 request_timeout=15,     # ì‘ë‹µ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
                 embedding_model: str = "korean",
                 vector_db_type: str = "chroma"):
        """
        RAGPipeline ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸
            temperature (float): ìƒì„± ì°½ì˜ì„± ì¡°ì ˆ (0.0~1.0)
            max_tokens (int): ìµœëŒ€ ì‘ë‹µ í† í° ìˆ˜
            embedding_model (str): ì„ë² ë”© ëª¨ë¸ íƒ€ì…
            vector_db_type (str): ë²¡í„° DB íƒ€ì…
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # OpenAI ChatGPT ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.vector_manager = VectorStoreManager(
            embedding_model=embedding_model,
            vector_db_type=vector_db_type,
            persist_directory=os.path.join(os.getcwd(), "data", "vector_db")  # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        self.prompt_template = self._create_prompt_template()
        
        # QA ì²´ì¸ (ë‚˜ì¤‘ì— ì´ˆê¸°í™”)
        self.qa_chain = None
        
        print(f"RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"  - LLM ëª¨ë¸: {model_name}")
        print(f"  - Temperature: {temperature}")
        print(f"  - ì„ë² ë”©: {embedding_model}")
        print(f"  - ë²¡í„° DB: {vector_db_type}")
    
    def _create_prompt_template(self) -> PromptTemplate:
        """
        í•™êµ ê³µì§€ì‚¬í•­ íŠ¹í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        
        Returns:
            PromptTemplate: ì„¤ì •ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        """
        template = """ë‹¹ì‹ ì€ ê²½êµ­ëŒ€í•™êµì˜ AI ê³µì§€ì‚¬í•­ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
í•™ìƒë“¤ì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

=== ê´€ë ¨ ê³µì§€ì‚¬í•­ ===
{context}

=== âš ï¸ ì¤‘ìš”í•œ ë‹µë³€ ê·œì¹™ ===
1. **ë°˜ë“œì‹œ ì œê³µëœ ê³µì§€ì‚¬í•­ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”**
2. **ì œê³µëœ ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”**
3. **ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”**
4. **ë‹¤ë¥¸ ì •ë³´ì™€ í˜¼ë™í•˜ì—¬ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”**
5. ë‹µë³€ ëì— ê´€ë ¨ ë¶€ì„œ ì—°ë½ì²˜ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì•ˆë‚´í•˜ì„¸ìš”  ğŸ“
6. ì¹œê·¼í•˜ê³  ì •ì¤‘í•œ í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
7. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
8. ë‹µë³€ì€ 300ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
9. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš” ğŸ“
10. ì¤‘ìš”í•œ ì •ë³´ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
    
    def setup_rag_system(self, force_rebuild: bool = False) -> bool:
        """
        RAG ì‹œìŠ¤í…œì„ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            force_rebuild (bool): ê°•ì œë¡œ ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì¶• ì—¬ë¶€
            
        Returns:
            bool: ì„¤ì • ì„±ê³µ ì—¬ë¶€
        """
        try:
            print("=== RAG ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ===")
            
            # SafeVectorStoreManagerì˜ ë©”ì„œë“œì— ë§ì¶° ìˆ˜ì •
            if not force_rebuild:
                # ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
                # ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
                # ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸
                if os.path.exists(self.vector_manager.persist_directory):
                    print("âœ… ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ ë°œê²¬")
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ ì°¾ê¸°
                    try:
                        import chromadb
                        client = chromadb.PersistentClient(path=str(self.vector_manager.persist_directory))
                        collections = client.list_collections()
                        
                        # ë¬¸ì„œê°€ ìˆëŠ” ì²« ë²ˆì§¸ ì»¬ë ‰ì…˜ ì‚¬ìš©
                        for collection in collections:
                            count = collection.count()
                            if count > 0:
                                print(f"âœ… '{collection.name}' ì»¬ë ‰ì…˜ ë°œê²¬ ({count}ê°œ ë¬¸ì„œ)")
                                
                                from langchain_chroma import Chroma
                                self.vector_manager.vector_store = Chroma(
                                    persist_directory=str(self.vector_manager.persist_directory),
                                    embedding_function=self.vector_manager.embeddings,
                                    collection_name=collection.name  # ì‹¤ì œ ì»¬ë ‰ì…˜ ì´ë¦„ ì‚¬ìš©
                                )
                                print("âœ… ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ")
                                break
                        else:
                            print("âŒ ë¬¸ì„œê°€ ìˆëŠ” ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            force_rebuild = True
                            
                    except Exception as e:
                        print(f"ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                        force_rebuild = True
                else:
                    print("ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    force_rebuild = True
            
            if force_rebuild or not self.vector_manager.vector_store:
                # ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
                print("ğŸ”„ ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...")
                
                # ë¬¸ì„œ ì²˜ë¦¬
                processor = DocumentProcessor()
                chunks = processor.process_documents()
                
                if not chunks:
                    print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                # ë²¡í„° ì €ì¥ì†Œ ìƒì„± (SafeVectorStoreManagerì˜ ë©”ì„œë“œ ì‚¬ìš©)
                self.vector_manager.create_vector_store_safe(chunks)
                print("âœ… ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ")
            
            # ë²¡í„° ì €ì¥ì†Œê°€ ì œëŒ€ë¡œ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if not self.vector_manager.vector_store:
                print("âŒ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False
            
            # ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸
            print("ğŸ” ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸ ì¤‘...")
            try:
                test_results = self.vector_manager.vector_store.similarity_search("í…ŒìŠ¤íŠ¸", k=1)
                print(f"ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œ ìˆ˜ í™•ì¸: {len(test_results)}ê°œ ë¬¸ì„œ ë°œê²¬")
                if test_results:
                    print(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ì œëª©: {test_results[0].metadata.get('title', 'N/A')}")
                else:
                    print("âš ï¸ ë²¡í„° ì €ì¥ì†Œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            except Exception as e:
                print(f"âš ï¸ ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            # 3. QA ì²´ì¸ êµ¬ì„±
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",  # ëª¨ë“  ê´€ë ¨ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
                retriever=self.vector_manager.vector_store.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 3}  # ìƒìœ„ 3ê°œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
                ),
                chain_type_kwargs={
                    "prompt": self.prompt_template,
                    "verbose": True  # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ì¶œë ¥
                },
                return_source_documents=True  # ì¶œì²˜ ë¬¸ì„œ ë°˜í™˜
            )
            
            print("âœ… RAG ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ RAG ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            question (str): ì‚¬ìš©ì ì§ˆë¬¸
            
        Returns:
            Dict: ë‹µë³€ ê²°ê³¼ (ë‹µë³€, ì¶œì²˜, ë©”íƒ€ë°ì´í„° í¬í•¨)
        """
        if not self.qa_chain:
            return {
                "answer": "RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "source_documents": [],
                "error": True
            }
        
        try:
            print(f"\nğŸ” ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: '{question}'")
            
            # ì¶œë ¥ ìˆ¨ê¸°ê¸°
            import contextlib
            import io
            
            with contextlib.redirect_stdout(io.StringIO()):
                result = self.qa_chain.invoke({"query": question})
            
            # ê²°ê³¼ ì •ë¦¬
            answer = result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            source_docs = result.get("source_documents", [])
            
            # ì¶œì²˜ ì •ë³´ ì •ë¦¬ (ê°„ì†Œí™”)
            sources = []
            for doc in source_docs:
                source_info = {
                    "title": doc.metadata.get("title", "ì œëª© ì—†ìŒ"),
                    "category": doc.metadata.get("category", "ë¶„ë¥˜ ì—†ìŒ"),
                    "date": doc.metadata.get("date", "ë‚ ì§œ ì—†ìŒ")
                    # content_preview ì œê±°
                }
                sources.append(source_info)
            
            response = {
                "answer": answer,
                "source_documents": sources,
                "total_sources": len(sources),
                "timestamp": datetime.now().isoformat(),
                "error": False
            }
            
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (ì¶œì²˜: {len(sources)}ê°œ)")
            return response
            
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "answer": f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "source_documents": [],
                "error": True
            }
            
    def batch_ask_questions(self, questions: List[str]) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì§ˆë¬¸ì„ ì¼ê´„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            questions (List[str]): ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict]: ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, question in enumerate(questions, 1):
            print(f"\n=== ì§ˆë¬¸ {i}/{len(questions)} ===")
            result = self.ask_question(question)
            result["question"] = question
            results.append(result)
        
            # API ì†ë„ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            if i < len(questions):  # ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´
                print("â³ API ì•ˆì •í™”ë¥¼ ìœ„í•´ 2ì´ˆ ëŒ€ê¸°...")
                time.sleep(2)
        
        return results
    
    def get_similar_documents(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜ (LLM ì—†ì´ ìˆœìˆ˜ ê²€ìƒ‰ë§Œ)
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            k (int): ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            List[Dict]: ìœ ì‚¬ ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        if not self.vector_manager.vector_store:
            return []
        
        try:
            # SafeVectorStoreManagerì˜ ë©”ì„œë“œ ì‚¬ìš©
            results = self.vector_manager.similarity_search_with_score(query, k=k)
            
            similar_docs = []
            for doc, score in results:
                doc_info = {
                    "title": doc.metadata.get("title", "ì œëª© ì—†ìŒ"),
                    "category": doc.metadata.get("category", "ë¶„ë¥˜ ì—†ìŒ"),
                    "date": doc.metadata.get("date", "ë‚ ì§œ ì—†ìŒ"),
                    "similarity_score": float(score),
                    "content_preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                similar_docs.append(doc_info)
            
            return similar_docs
            
        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def evaluate_system(self, test_questions: List[str]) -> Dict[str, Any]:
        """
        ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            test_questions (List[str]): í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
            
        Returns:
            Dict: í‰ê°€ ê²°ê³¼
        """
        if not self.qa_chain:
            return {"error": "RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        print("=== ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ì¤‘ ===")
        
        results = self.batch_ask_questions(test_questions)
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        total_questions = len(results)
        successful_answers = len([r for r in results if not r.get("error", False)])
        avg_sources = sum(r.get("total_sources", 0) for r in results) / total_questions if total_questions > 0 else 0
        
        evaluation = {
            "total_questions": total_questions,
            "successful_answers": successful_answers,
            "success_rate": successful_answers / total_questions if total_questions > 0 else 0,
            "average_sources_per_answer": avg_sources,
            "detailed_results": results
        }
        
        print(f"ğŸ“Š í‰ê°€ ì™„ë£Œ:")
        print(f"  - ì´ ì§ˆë¬¸ ìˆ˜: {total_questions}")
        print(f"  - ì„±ê³µí•œ ë‹µë³€: {successful_answers}")
        print(f"  - ì„±ê³µë¥ : {evaluation['success_rate']:.1%}")
        print(f"  - í‰ê·  ì¶œì²˜ ìˆ˜: {avg_sources:.1f}")
        
        return evaluation
    
    def update_system_prompt(self, new_template: str):
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            new_template (str): ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        """
        self.prompt_template = PromptTemplate(
            template=new_template,
            input_variables=["context", "question"]
        )
        
        # QA ì²´ì¸ ì¬êµ¬ì„±
        if self.qa_chain:
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vector_manager.vector_store.as_retriever(
                    search_kwargs={"k": 3}
                ),
                    chain_type_kwargs={
                        "prompt": self.prompt_template,
                        "verbose": False  # verbose ë„ê¸°!
                    },
                return_source_documents=True
            )
        
        print("âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì—…ë°ì´íŠ¸ ì™„ë£Œ")


def main():
    """
    RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    """
    print("=== RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    rag = RAGPipeline(
        model_name="gpt-3.5-turbo",
        temperature=0.1,
        max_tokens=300,  # ë” ì§§ì€ ë‹µë³€ìœ¼ë¡œ ì†ë„ í–¥ìƒ
        request_timeout=30
    )
    
    # RAG ì‹œìŠ¤í…œ ì„¤ì •
    if not rag.setup_rag_system():
        print("RAG ì‹œìŠ¤í…œ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì¥í•™ê¸ˆ ì‹ ì²­ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ë„ì„œê´€ ìš´ì˜ì‹œê°„ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "í•´ì™¸êµí™˜í•™ìƒ í”„ë¡œê·¸ë¨ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "ì·¨ì—…ë°•ëŒíšŒëŠ” ì–¸ì œ ì—´ë¦¬ë‚˜ìš”?",
        "í•™ìƒì‹ë‹¹ ë©”ë‰´ê°€ ë³€ê²½ë˜ì—ˆë‚˜ìš”?",
        "ê¸°ìˆ™ì‚¬ ì‹ ì²­ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"  # ë°ì´í„°ì— ì—†ëŠ” ì§ˆë¬¸
    ]
    
    # ê°œë³„ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ (ì „ì²´ 6ê°œ)
    print("\n=== ê°œë³„ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ ===")
    for question in test_questions:  # [:3] ì œê±° â†’ ì „ì²´ 6ê°œ
        result = rag.ask_question(question)
        
        print(f"\nì§ˆë¬¸: {question}")
        print(f"ë‹µë³€: {result['answer']}")
        print(f"ì¶œì²˜ ìˆ˜: {result['total_sources']}")
        
        if result['source_documents']:
            print("ê´€ë ¨ ë¬¸ì„œ:")
            for i, source in enumerate(result['source_documents'], 1):
                print(f"  {i}. {source['title']} ({source['category']})")
    
    # ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€
    print("\n=== ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ===")
    evaluation = rag.evaluate_system(test_questions)
    
    print(f"ì „ì²´ ì„±ê³µë¥ : {evaluation['success_rate']:.1%}")
    
    print("\n=== RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")


if __name__ == "__main__":
    main()